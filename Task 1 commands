Initialize KAFKA Server:

#Using downloaded files

1.Generate a Cluster UUID

   $ KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"

2.Format Log Directories

   $ bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties

3.Start the Kafka Server

   $ bin/kafka-server-start.sh config/kraft/server.properties
   
Create a TOPIC (in new terminal)

   $ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092 PartitionCount: 5  ReplicationFactor: 1
   
Writing Events in Topic

   $ bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
   
Reading Events (in new terminal)

   $ bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092
   
Writing Data in KAFKA server using Pyspark

   $ spark-submit --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1 /home/xs442-tarsin/kafka/hello/KafkaRead.py
   
Writing Data from KAFKA server to console using Pyspark

   $ spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.apache.spark:spark-streaming-kafka_2.11:1.6.3,io.delta:delta-spark_2.12:3.1.0 --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" /home/xs442-tarsin/kafka/hello/KafkaRead.py
